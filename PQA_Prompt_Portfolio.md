# Precision QA Evaluator (PQA)
**Prompt Engineering Portfolio Entry – Accuracy & Ambiguity Scoring Framework**

---

## Overview

The **Precision QA Evaluator (PQA)** is a prompt system designed to assess the **clarity, correctness, and ambiguity** of answers generated by language models. It turns a standard QA interaction into a **self-checking process** by prompting the model to reflect on its own output using precision metrics.

This is particularly useful for improving **response reliability** in knowledge-based applications such as customer service bots, AI assistants, or critical compliance tools.

---

## Use Case Example

### Use Case: AI Assistant Self-Auditing

The prompt was used to evaluate a knowledge assistant for an insurance company, testing whether its answers to policyholder questions were clear, precise, and aligned with documented rules.

**Prompted Question**:
> “Does my policy cover damage from a neighbor's tree falling on my house?”

**Model Answer**:
> “It depends on your policy terms, but generally, your homeowners insurance may cover this if the tree was healthy and fell due to a storm.”

**PQA Evaluation Prompt**:
> Evaluate the above response:
> - Score for factual precision (1–5)
> - Identify any ambiguous terms
> - Highlight unstated assumptions
> - Suggest a more precise revision

**PQA Output**:
- **Precision Score**: 3/5  
- **Ambiguous Terms**: “generally,” “may cover,” “if the tree was healthy”  
- **Unstated Assumptions**: Assumes standard homeowners insurance, no exclusions  
- **Suggested Revision**:  
> “If your homeowners insurance includes storm damage coverage and the tree was not previously reported as a hazard, your policy is likely to cover the damage. Exact terms vary by insurer.”

---

## Prompt Structure

\`\`\`text
ROLE: Precision QA Evaluator  
OBJECTIVE: Identify ambiguity, clarify assumptions, and score factual alignment in generated answers.  

INPUT INSTRUCTIONS:  
For any answer provided, analyze the following:  
1. Assign a factual precision score (1 = vague/inaccurate, 5 = highly accurate/specific)  
2. List ambiguous or undefined terms  
3. Identify assumptions or unstated variables  
4. Provide a more precise and context-aware version of the original answer

OUTPUT FORMAT:  
- Precision Score:  
- Ambiguous Terms:  
- Unstated Assumptions:  
- Suggested Revision:

SAMPLE QUESTION: [Insert QA pair here]
\`\`\`

---

## Why It Works

- Turns LLMs into **self-auditing agents**, improving trust in answers
- Helps companies develop **high-quality QA datasets** with better labeling
- Excellent for industries where **compliance, accuracy, and tone matter**

---

## Reflection

This prompt showcases how **meta-prompting** can enhance the usefulness and accountability of LLM-generated outputs. It’s especially relevant for real-world deployment, where unclear or incorrect answers can lead to customer frustration, liability, or lost trust.

The framework can also be repurposed as part of a **toolchain for AI safety**, where model outputs must be evaluated before delivery to users.
